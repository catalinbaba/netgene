<!DOCTYPE HTML>
<!-- NewPage -->
<html lang="en">
<head>
<!-- Generated by javadoc (16) on Fri Mar 29 18:39:32 EET 2024 -->
<title>org.netgene.network.learning.optimizer</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="dc.created" content="2024-03-29">
<meta name="description" content="declaration: package: org.netgene.network.learning.optimizer">
<meta name="generator" content="javadoc/PackageWriterImpl">
<link rel="stylesheet" type="text/css" href="../../../../../stylesheet.css" title="Style">
<link rel="stylesheet" type="text/css" href="../../../../../script-dir/jquery-ui.min.css" title="Style">
<link rel="stylesheet" type="text/css" href="../../../../../jquery-ui.overrides.css" title="Style">
<script type="text/javascript" src="../../../../../script.js"></script>
<script type="text/javascript" src="../../../../../script-dir/jquery-3.5.1.min.js"></script>
<script type="text/javascript" src="../../../../../script-dir/jquery-ui.min.js"></script>
</head>
<body class="package-declaration-page">
<script type="text/javascript">var pathtoroot = "../../../../../";
loadScripts(document, 'script');</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<div class="flex-box">
<header role="banner" class="flex-header">
<nav role="navigation">
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="top-nav" id="navbar.top">
<div class="skip-nav"><a href="#skip.navbar.top" title="Skip navigation links">Skip navigation links</a></div>
<ul id="navbar.top.firstrow" class="nav-list" title="Navigation">
<li><a href="../../../../../index.html">Overview</a></li>
<li class="nav-bar-cell1-rev">Package</li>
<li>Class</li>
<li><a href="package-use.html">Use</a></li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../../index-files/index-1.html">Index</a></li>
<li><a href="../../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="sub-nav">
<div class="nav-list-search"><label for="search">SEARCH:</label>
<input type="text" id="search" value="search" disabled="disabled">
<input type="reset" id="reset" value="reset" disabled="disabled">
</div>
</div>
<!-- ========= END OF TOP NAVBAR ========= -->
<span class="skip-nav" id="skip.navbar.top">
<!--   -->
</span></nav>
</header>
<div class="flex-content">
<main role="main">
<div class="header">
<h1 title="Package" class="title">Package&nbsp;org.netgene.network.learning.optimizer</h1>
</div>
<hr>
<div class="package-signature">package <span class="element-name">org.netgene.network.learning.optimizer</span></div>
<section class="summary">
<ul class="summary-list">
<li>
<div class="caption"><span>Class Summary</span></div>
<div class="summary-table two-column-summary">
<div class="table-header col-first">Class</div>
<div class="table-header col-last">Description</div>
<div class="col-first even-row-color"><a href="Adadelta.html" title="class in org.netgene.network.learning.optimizer">Adadelta</a></div>
<div class="col-last even-row-color">
<div class="block">Like RMSprop, Adadelta (Zeiler, 2012) is also another improvement from AdaGrad, focusing on the learning rate component.</div>
</div>
<div class="col-first odd-row-color"><a href="AdaGrad.html" title="class in org.netgene.network.learning.optimizer">AdaGrad</a></div>
<div class="col-last odd-row-color">
<div class="block">Adaptive gradient, or AdaGrad (Duchi et al., 2011), acts on the learning rate component by dividing the learning rate by the square root of v, 
 which is the cumulative sum of current and past squared gradients (i.e.</div>
</div>
<div class="col-first even-row-color"><a href="Adam.html" title="class in org.netgene.network.learning.optimizer">Adam</a></div>
<div class="col-last even-row-color">
<div class="block">Adaptive moment estimation, or Adam (Kingma and Ba, 2014), is simply a combination of momentum and RMSprop.</div>
</div>
<div class="col-first odd-row-color"><a href="AdaMax.html" title="class in org.netgene.network.learning.optimizer">AdaMax</a></div>
<div class="col-last odd-row-color">
<div class="block">AdaMax (Kingma and Ba, 2015) is an adaptation of the Adam optimiser by the same authors using infinity norms (hence ‘max’).</div>
</div>
<div class="col-first even-row-color"><a href="AMSGrad.html" title="class in org.netgene.network.learning.optimizer">AMSGrad</a></div>
<div class="col-last even-row-color">
<div class="block">Another variant of Adam is the AMSGrad (Reddi et al., 2018).</div>
</div>
<div class="col-first odd-row-color"><a href="iRPROPm.html" title="class in org.netgene.network.learning.optimizer">iRPROPm</a></div>
<div class="col-last odd-row-color">
<div class="block">New weight back tracking method, some consider this to be the most advanced RPROP.</div>
</div>
<div class="col-first even-row-color"><a href="iRPROPp.html" title="class in org.netgene.network.learning.optimizer">iRPROPp</a></div>
<div class="col-last even-row-color">
<div class="block">New RPROP without weight back tracking.</div>
</div>
<div class="col-first odd-row-color"><a href="MillerSGD.html" title="class in org.netgene.network.learning.optimizer">MillerSGD</a></div>
<div class="col-last odd-row-color">
<div class="block">Similar to Stochastic Gradient Descent.</div>
</div>
<div class="col-first even-row-color"><a href="MSGD.html" title="class in org.netgene.network.learning.optimizer">MSGD</a></div>
<div class="col-last even-row-color">
<div class="block">Instead of depending only on the current gradient to update the weight, gradient descent with momentum (Polyak, 1964) 
 replaces the current gradient with m (“momentum”), which is an aggregate of gradients.</div>
</div>
<div class="col-first odd-row-color"><a href="Nadam.html" title="class in org.netgene.network.learning.optimizer">Nadam</a></div>
<div class="col-last odd-row-color">
<div class="block">Nadam (Dozat, 2015) is an acronym for Nesterov and Adam optimiser.</div>
</div>
<div class="col-first even-row-color"><a href="NAG.html" title="class in org.netgene.network.learning.optimizer">NAG</a></div>
<div class="col-last even-row-color">
<div class="block">Nesterov Accelerated Gradient (Sutskever et al., 2013).</div>
</div>
<div class="col-first odd-row-color"><a href="RMSprop.html" title="class in org.netgene.network.learning.optimizer">RMSprop</a></div>
<div class="col-last odd-row-color">
<div class="block">Root mean square prop or RMSprop (Hinton et al., 2012) is another adaptive learning rate that tries to improve AdaGrad.</div>
</div>
<div class="col-first even-row-color"><a href="RPROP.html" title="class in org.netgene.network.learning.optimizer">RPROP</a></div>
<div class="col-last even-row-color">
<div class="block">Rprop stands for 'Resilient backpropagation' and is a local adaptive learning scheme, performing supervised batch learning in multi-layer perceptrons.</div>
</div>
<div class="col-first odd-row-color"><a href="RPROPm.html" title="class in org.netgene.network.learning.optimizer">RPROPm</a></div>
<div class="col-last odd-row-color">
<div class="block">No weight back tracking.</div>
</div>
<div class="col-first even-row-color"><a href="RPROPp.html" title="class in org.netgene.network.learning.optimizer">RPROPp</a></div>
<div class="col-last even-row-color">
<div class="block">The classic RPROP algorithm.</div>
</div>
<div class="col-first odd-row-color"><a href="SGD.html" title="class in org.netgene.network.learning.optimizer">SGD</a></div>
<div class="col-last odd-row-color">
<div class="block">Gradient descent is an optimization method for finding the minimum of a function.</div>
</div>
</div>
</li>
</ul>
</section>
</main>
</div>
</div>
</body>
</html>
